# üöÄ DeepSeek Workshop

Welcome to the **DeepSeek Workshop**, a collaborative project between **Sean Stephenson** and **ChatGPT**! ü§ù This project is all about exploring **cutting-edge AI models** and fine-tuning them for **practical text generation**.

## üìå Overview
This project demonstrates how to:
- Load and interact with **DeepSeek AI models** using Hugging Face's `transformers` library.
- Optimize model performance on different hardware (Mac MPS, CPU, etc.).
- Handle potential **out-of-memory (OOM)** issues with best practices.
- **Format model outputs** into a nicely styled **HTML page** for easy review.

---

## üìú Running the Script


## Running the Project üöÄ

Follow these steps to set up and run the project locally:

### 1Ô∏è‚É£ Clone the Repository
```bash
git clone git@github.com:SeanPlusPlus/deepseek-workshop.git
cd deepseek-workshop
```

### 2Ô∏è‚É£ Set Up a Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3Ô∏è‚É£ Install Dependencies
```bash
pip install --upgrade pip  # Ensure pip is up-to-date
pip install -r requirements.txt
```

### 4Ô∏è‚É£ Run the Script
```bash
python hello_deepseek.py
```

This will:
- Load the **DeepSeek-R1-Distill-Qwen-1.5B** model.
- Run an inference on a sample prompt.
- Generate and save results into a formatted HTML file.

### 5Ô∏è‚É£ View the Output
The output will be saved as an HTML file and automatically opened in your default browser.

If you run into memory issues, try switching to a **smaller model** or using CPU mode by adjusting `device = "cpu"` in the script.

---

## üîç **Diving Deeper: Understanding `hello_deepseek.py`**
To **fully grasp how the model works**, we **highly encourage** you to review the **extensive comments** in `hello_deepseek.py`.  

This script **not only runs the model** but also explains:

‚úÖ **What DeepSeek AI is and how it works**  
‚úÖ **Why we use specific optimizations** (e.g., `torch_dtype`, `device_map`, `offload_folder`)  
‚úÖ **How tokenization transforms text into numbers and back**  
‚úÖ **How to adjust model settings to prevent memory issues**  
‚úÖ **Why using `torch.no_grad()` helps save memory during inference**  
‚úÖ **How we structure example prompts to test various AI capabilities**  

Reading through `hello_deepseek.py` will give you **insight into AI model deployment** and help you debug or customize the script for your own use.

---

## üé® **Formatting Model Outputs as HTML**
The generated AI responses are automatically saved into an **HTML file** for a clean and shareable output.  

To modify how this works:
- Check out `html_formatter.py`, which takes model outputs and creates a **Bootstrap-styled** web page.
- Feel free to **customize the CSS and layout** as needed!

---

## ‚ö†Ô∏è **Troubleshooting & Performance Tuning**
We hit **a ton of roadblocks** while getting this to work, and we want to make sure you don't have to struggle as much! Here are the major lessons learned:

### **1Ô∏è‚É£ Resolving Model Authentication Issues** üîë
- We initially hit 401 and 404 errors when trying to load DeepSeek models.
- Solution: We needed to **log in to Hugging Face** and use an **access token**.
  ```bash
  huggingface-cli login
  ```
  Then, visit [Hugging Face Tokens](https://huggingface.co/settings/tokens) to generate an access token.

### **2Ô∏è‚É£ Fixing Out-of-Memory (OOM) Errors** üöÄ
- Running larger models caused memory crashes on **Mac MPS (Metal Performance Shaders)**.
- Solutions:
  - **Use a smaller model**: If `"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"` fails, try `"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.3B"`.
  - **Lower `max_new_tokens`** in `model.generate()`.
  - **Offload to CPU**: If necessary, manually set `device = "cpu"`.

### **3Ô∏è‚É£ Bitsandbytes & PyTorch Compatibility** üèóÔ∏è
- Installing `bitsandbytes` was tricky due to **NumPy version conflicts**.
- Solution:
  ```bash
  pip install --no-cache-dir --force-reinstall numpy==1.26.4 bitsandbytes torch
  ```
- If using MPS, ensure `torch==2.2.2` is installed!

### **4Ô∏è‚É£ Debugging Torch Device Errors** üî•
- If `MPS` backend gives errors:
  ```bash
  PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 python hello_deepseek.py
  ```
  This disables memory limits (use cautiously!).

---

## Example Output

<img width="740" alt="image" src="https://github.com/user-attachments/assets/54644eac-660a-4ef7-ac4c-1d39057b644e" />

---

## üî• Next Steps
- **Run the script** and see what AI-generated text it produces!  
- **Modify prompts** to test different types of responses.  
- **Experiment with different models** from Hugging Face.  

Let us know if you have any questions or improvements! üöÄüí°